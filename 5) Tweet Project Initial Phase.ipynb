{"cells":[{"cell_type":"markdown","metadata":{"id":"lVp6YZE8qPLj"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lh1fMgG9GN9"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import string\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from numpy.lib.type_check import nan_to_num\n","from numpy.core.numeric import NaN\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","import warnings\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1208,"status":"ok","timestamp":1690325382451,"user":{"displayName":"Arthik Alexander","userId":"10019163061696473988"},"user_tz":240},"id":"UTC0I7y2If-6","outputId":"8b5d5a32-d1e8-49c2-9df4-07ae3c4bc5bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkWjHrf_0GBb"},"outputs":[],"source":["warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"PTM6VSvXq5Zy"},"source":["## Import Dataset from Local"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":39},"id":"R2l7ePykAjRC","outputId":"57e2feef-b207-4bcc-8380-b4452b33025a"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-f30f43a2-ba20-409b-87ac-2252a9a93f44\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f30f43a2-ba20-409b-87ac-2252a9a93f44\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","file = files.upload()  #upload file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62RBridgfxL4"},"outputs":[],"source":["df = pd.read_csv(\"Tweets.csv\")"]},{"cell_type":"markdown","metadata":{"id":"yLPuaU0hq8vS"},"source":["## Initial Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRBu9JgNBaDC"},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{"id":"n5zCyPqlr-oV"},"source":["The describe() functions provides us with the metrics of the numerical variables. Perhaps the most interesting is the count columns. Because we have a number of categorical variables too, we will be examining the shape and type info next."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31PnXRf4Bln4"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"IDeO3Rjorvkf"},"source":["We see that there are 15 total features with 1 of the features being the ID of the tweet. Of those, there are quite a few features that have many rows missing. We also see that the tweet_created column is an object when it should be a datetime type."]},{"cell_type":"markdown","metadata":{"id":"KnXUOhNqrX36"},"source":["#### Examine First 5 Rows of Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGKr9GTkBpJV"},"outputs":[],"source":["df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"RzmM_h49rdqv"},"source":["In the previous section, we saw the types of the different features, but here, we're able to see the actual values in the rows. For example, it looks like the airline_sentiment column provides information such as positive, negative, neutral.  "]},{"cell_type":"markdown","metadata":{"id":"iECxX2zW4exV"},"source":["#### Examining Unique Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IewOS1_z4hBP"},"outputs":[],"source":["df.nunique()"]},{"cell_type":"markdown","metadata":{"id":"aKEEMsK-4kJd"},"source":["From the above, we see that there are three general sentiments which we saw previously (positive, negative, neutral). It's also interesting to note that out of all fo the tweets, we are specifically looking at 6 airlines. Here are the 6 airlines whose tweets we will be examining:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaGr8qSK4yUP"},"outputs":[],"source":["df['airline'].unique()"]},{"cell_type":"markdown","metadata":{"id":"7d7LGeSprT2c"},"source":["#### Check for Nulls/NaNs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJI0VzlPCb4m"},"outputs":[],"source":["num_of_nan_missing=df.isnull().sum()\n","print(num_of_nan_missing)"]},{"cell_type":"markdown","metadata":{"id":"DJw-NHZprJXh"},"source":["Based on the above, we see that there are a substantial number of rows missing for the different features in our dataset, however we need to figure out which columns are relevant to our analysis."]},{"cell_type":"markdown","metadata":{"id":"tUEM7yrOrRAs"},"source":["## Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"DAm2uIol3sCp"},"source":["#### Configure Datetime Objects"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hViff2GZgWP0"},"outputs":[],"source":["df['tweet_created'] = pd.to_datetime(df['tweet_created'])\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"trDbuLpU3sH6"},"source":["Now we see that the tweet_created feature is of type datetime64 which will make it easier for datetime analysis in the exploration phase."]},{"cell_type":"markdown","metadata":{"id":"K46nPUAe3sKl"},"source":["#### Handling the Null Values"]},{"cell_type":"markdown","metadata":{"id":"sAvqR3U-nZmR"},"source":["First we're going to find the percentage of features whose values are null."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lt0hLWK1nIUo"},"outputs":[],"source":["df.isnull().mean()*100"]},{"cell_type":"markdown","metadata":{"id":"o99ntvp-3sM9"},"source":["We see that negativereason_gold, airline_sentiment_gold, and tweet_coord have over 90% of null values, therefore we will drop those."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NjJkYLOoD2q"},"outputs":[],"source":["df = df.drop(df.columns[df.isnull().mean()>0.90], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"WOoJopMsorH2"},"source":["After dropping the values, we will validate that they have been dropped successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLXK4MvMopn9"},"outputs":[],"source":["df.isnull().mean()*100"]},{"cell_type":"markdown","metadata":{"id":"xS4fLR-eo7t8"},"source":["#### Handling the Text Columns"]},{"cell_type":"markdown","metadata":{"id":"d7RNMA-z0vEo"},"source":["First we created new columns to separate the Twitter handle from the rest of the tweet."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Wdi7j7mwWsPk","executionInfo":{"status":"error","timestamp":1692038251761,"user_tz":240,"elapsed":342,"user":{"displayName":"Arthik Alexander","userId":"10019163061696473988"}},"outputId":"7c9259d8-bd1b-467d-c5b6-bebdb07015c1","colab":{"base_uri":"https://localhost:8080/","height":211}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fb34fb6af90c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'first_word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'remaining_sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["df['first_word'] = df['text'].str.split(' ', 1).str[0]\n","df.loc[~df['first_word'].str.startswith('@'), 'first_word'] = np.nan\n","df['remaining_sentence'] = df['text'].str.split(' ', 1).str[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGkTKfJXW0uD","executionInfo":{"status":"aborted","timestamp":1692038251762,"user_tz":240,"elapsed":4,"user":{"displayName":"Arthik Alexander","userId":"10019163061696473988"}}},"outputs":[],"source":["print(df['first_word'])"]},{"cell_type":"markdown","metadata":{"id":"A2R3HJCR0V4S"},"source":["This is important to the analysis because we want to begin cleaning the tweets to retrieve the sentiment-sensitive text. We'll take a look at the twitter handles we just retrieved."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVThpKm9X6nO","executionInfo":{"status":"aborted","timestamp":1692038251762,"user_tz":240,"elapsed":4,"user":{"displayName":"Arthik Alexander","userId":"10019163061696473988"}}},"outputs":[],"source":["unique_values = df['first_word'].unique()\n","print(unique_values)"]},{"cell_type":"markdown","metadata":{"id":"Zpus97zu00VI"},"source":["Above we see that there are slight variations in the twitter handles due to differences in capitalization and and punctuation. We can assume that the rest of the tweet will have similar issues so next we must remove do things like convert text to lowercase, remove special characters and punctuation, and remove numbers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yi33SV-iJGjg","executionInfo":{"status":"aborted","timestamp":1692038251763,"user_tz":240,"elapsed":5,"user":{"displayName":"Arthik Alexander","userId":"10019163061696473988"}}},"outputs":[],"source":["# Assuming 'df' is your DataFrame and 'text_column' is the column containing the tweets\n","tweets = df['remaining_sentence']\n","\n","# Convert text to lowercase\n","tweets = tweets.str.lower()\n","\n","# Remove URLs\n","tweets = tweets.apply(lambda x: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", x))\n","\n","# Remove special characters and punctuation\n","tweets = tweets.apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", x))\n","\n","# Remove digits\n","tweets = tweets.apply(lambda x: re.sub(r\"\\d+\", \"\", x))\n","\n","# Tokenize the tweets\n","tweets = tweets.apply(word_tokenize)\n","\n","# Remove stop words\n","stop_words = set(stopwords.words(\"english\"))\n","tweets = tweets.apply(lambda x: [word for word in x if word not in stop_words])\n","\n","# Lemmatize the tokens\n","lemmatizer = WordNetLemmatizer()\n","tweets = tweets.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Join the tokens back into sentences\n","tweets = tweets.apply(lambda x: ' '.join(x))\n","\n","# Update the DataFrame with the cleaned tweets\n","df['cleaned_tweets'] = tweets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NjBSmMrKGIq","executionInfo":{"status":"aborted","timestamp":1692038251763,"user_tz":240,"elapsed":5,"user":{"displayName":"Arthik Alexander","userId":"10019163061696473988"}}},"outputs":[],"source":["df['cleaned_tweets']"]},{"cell_type":"markdown","metadata":{"id":"EOmDCsRCwohR"},"source":["Above we see that the tweets have a lot of the unwanted text removed alongside standardized capitalization."]},{"cell_type":"markdown","metadata":{"id":"uznZb6EHwwoA"},"source":["#### Initial Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"H5V8qJyRwyw8"},"source":["For our preliminary exploration into the sentiment analysis, we're going to start off by calculating the sentiment scores so we can understand what our baseline is."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGp8jPcyHHFa"},"outputs":[],"source":["cleaned_tweets = df['cleaned_tweets']\n","\n","sid = SentimentIntensityAnalyzer()\n","\n","sentiment_scores = cleaned_tweets.apply(lambda x: sid.polarity_scores(x))\n","\n","compound_scores = sentiment_scores.apply(lambda x: x['compound'])\n","\n","df['sentiment_score'] = compound_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lb9AmfHZLJOa"},"outputs":[],"source":["df['sentiment_score']"]},{"cell_type":"markdown","metadata":{"id":"Y5H-o6MfxA9o"},"source":["Above we see the sentiment_score column that was added to our dataframe that we can now use for some preliminary EDA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzWavQcfU8z0"},"outputs":[],"source":["df_cleaned = df.copy()"]},{"cell_type":"markdown","metadata":{"id":"bpFNqofL26ZW"},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmTLtE2m29Mv"},"outputs":[],"source":["ax = df['airline'].value_counts().plot(kind='bar',\n","                                    figsize=(7,4),\n","                                    title=\"Count of Airline @s\")\n","ax.set_xlabel(\"Airline Name\")\n","ax.set_ylabel(\"Frequency\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-g7pV44IY8H"},"outputs":[],"source":["ax = df['negativereason'].value_counts().plot(kind='barh',\n","                                    figsize=(7,4),\n","                                    title=\"Count of Negative Comments by Reason\", color = 'red')\n","ax.invert_yaxis()\n","ax.set_xlabel(\"Negative Reason\")\n","ax.set_ylabel(\"Frequency\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"z3UCpYNSOd23"},"source":["IDEA\n","X axis - date\n","Y axis - avg sentiment score for the day by airline"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}